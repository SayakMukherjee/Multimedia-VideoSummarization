{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalProject.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1MEcu-H5dM2iATPs4ZMu52O01wkCvJDb_","authorship_tag":"ABX9TyMbAqmD0Gj9Eg7C7+CTdx5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Uncomment for colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ge-BmRCR8rA","executionInfo":{"status":"ok","timestamp":1654777194671,"user_tz":-120,"elapsed":9037,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"d641b1a2-e1d4-468c-dd32-ef2c32996c67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data.dataset import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"],"metadata":{"id":"BhbqBUA8-KwI","executionInfo":{"status":"ok","timestamp":1654785055719,"user_tz":-120,"elapsed":213,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["FEATURE_PATH = '/content/drive/MyDrive/MM/tvsum/features'\n","# video_dataset = ['-esJrBWj2d8', '98MoyGZKHXc']\n","video_dataset = ['98MoyGZKHXc']"],"metadata":{"id":"gVeK9l7fKjb-","executionInfo":{"status":"ok","timestamp":1654778967259,"user_tz":-120,"elapsed":225,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["param = {'VIDEO':0, 'AUDIO': 0, 'FRAMES':0}\n","\n","for video_id in video_dataset:\n","    for features in ['vggish', 'flow', 'rgb']:\n","        stack = np.load(os.path.join(FEATURE_PATH, f'{video_id}_{features}.npy'))\n","\n","        if features == 'vggish':\n","            if stack.shape[0] > param['AUDIO']:\n","                param['AUDIO'] = stack.shape[0]\n","\n","            if stack.shape[0] < param['FRAMES']:\n","                param['FRAMES'] = stack.shape[0]\n","        else:\n","            if stack.shape[0] > param['VIDEO']:\n","                param['VIDEO'] = stack.shape[0]\n","\n","            if stack.shape[0] < param['FRAMES']:\n","                param['FRAMES'] = stack.shape[0]\n","\n","print(param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I97fw1rM-pnd","executionInfo":{"status":"ok","timestamp":1654778968616,"user_tz":-120,"elapsed":19,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"16c3990b-a2ca-4621-99f3-cf6fc7b29e48"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{'VIDEO': 179, 'AUDIO': 195, 'FRAMES': 0}\n"]}]},{"cell_type":"code","source":["param['FRAMES'] = 179\n","param"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MggFpKAlmwCr","executionInfo":{"status":"ok","timestamp":1654778970956,"user_tz":-120,"elapsed":222,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"19026e20-cfbc-460f-84b0-b2def781183d"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AUDIO': 195, 'FRAMES': 179, 'VIDEO': 179}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["def rep_titles(features):\n","    new_stack = []\n","    new_stack.append(features[0])\n","    new_stack.extend(new_stack * param['FRAMES'])\n","    new_stack = torch.stack(new_stack)\n","    return new_stack\n","\n","def rep_captions(features):\n","    reps = param['FRAMES'] // features.shape[0]\n","    remainder = param['FRAMES'] % features.shape[0]\n","    new_stack = []\n","    for frame_ind in range(features.shape[0]):\n","        tmp = []\n","        tmp.append(features[frame_ind])\n","\n","        if frame_ind == features.shape[0] - 1:\n","            new_stack.extend(tmp * (reps + remainder))\n","        else:\n","            new_stack.extend(tmp * reps)\n","\n","    new_stack = torch.stack(new_stack)\n","    return new_stack\n","\n","def fill_missing_features(method, feature_size):\n","    if method == 'random':\n","        return torch.rand(1, feature_size)\n","    elif method == 'zero':\n","        return torch.zeros(1, feature_size).float()\n","\n","\n","def pad_segment(feature, max_feature_len, pad_idx):\n","    S, D = feature.shape\n","    assert S <= max_feature_len\n","    # pad\n","    l, r, t, b = 0, 0, 0, max_feature_len - S\n","    feature = F.pad(feature, [l, r, t, b], value=pad_idx)\n","    return feature\n","\n","def crop_a_segment(feature, start_idx, end_idx):\n","    S, D = feature.shape\n","\n","    # handles the case when a segment is too small\n","    if start_idx == end_idx:\n","        # if the small segment occurs in the end of a video\n","        # [S:S] -> [S-1:S]\n","        if start_idx == S:\n","            start_idx -= 1\n","        # [S:S] -> [S:S+1]\n","        else:\n","            end_idx += 1\n","    feature = feature[start_idx:end_idx, :]\n","\n","    if len(feature) == 0:\n","        return None\n","    else:\n","        return feature\n","\n","\n","def load_features_from_npy(feature_names_list, video_id,\n","                           pad_idx, get_full_feat=False):\n","\n","    supported_feature_names = {'i3d_features', 'vggish_features', 'caption_features', 'title_features'}\n","    assert isinstance(feature_names_list, list)\n","    assert len(feature_names_list) > 0\n","    assert set(feature_names_list).issubset(supported_feature_names)\n","\n","    stacks = {}\n","    if get_full_feat:\n","        stacks['orig_feat_length'] = {}\n","\n","    if 'vggish_features' in feature_names_list:\n","\n","        try:\n","            stack_vggish = np.load(os.path.join(FEATURE_PATH, f'{video_id}_vggish.npy'))\n","            stack_vggish = torch.from_numpy(stack_vggish).float()\n","\n","            if get_full_feat:\n","                stacks['orig_feat_length']['audio'] = stack_vggish.shape[0]\n","                if stack_vggish.shape[0] > param['FRAMES']:\n","                    stack_vggish = crop_a_segment(stack_vggish, start_idx = 0, end_idx = param['FRAMES'])\n","                else:\n","                    stack_vggish = pad_segment(stack_vggish, param['FRAMES'], pad_idx=0)\n","\n","        except FileNotFoundError as e:\n","            print(e)\n","            stack_vggish = None\n","\n","        stacks['audio'] = stack_vggish\n","\n","    # not elif\n","    if 'i3d_features' in feature_names_list:\n","        try:\n","            stack_rgb = np.load(os.path.join(FEATURE_PATH, f'{video_id}_rgb.npy'))\n","            stack_flow = np.load(os.path.join(FEATURE_PATH, f'{video_id}_flow.npy'))\n","            stack_rgb = torch.from_numpy(stack_rgb).float()\n","            stack_flow = torch.from_numpy(stack_flow).float()\n","\n","            assert stack_rgb.shape == stack_flow.shape\n","            if get_full_feat:\n","                stacks['orig_feat_length']['rgb'] = stack_rgb.shape[0]\n","                stacks['orig_feat_length']['flow'] = stack_flow.shape[0]\n","                \n","                if stack_rgb.shape[0] > param['FRAMES']:\n","                    stack_rgb = crop_a_segment(stack_rgb, start_idx = 0, end_idx = param['FRAMES'])\n","                else:\n","                    stack_rgb = pad_segment(stack_rgb, param['FRAMES'], pad_idx=0)\n","\n","                if stack_flow.shape[0] > param['FRAMES']:\n","                    stack_flow = crop_a_segment(stack_rgb, start_idx = 0, end_idx = param['FRAMES'])\n","                else:\n","                    stack_flow = pad_segment(stack_flow, param['FRAMES'], pad_idx=0)\n","\n","\n","                # stack_rgb = pad_segment(stack_rgb, param['VIDEO'], pad_idx=0)\n","                # stack_flow = pad_segment(stack_flow, param['VIDEO'], pad_idx=0)\n","\n","        except FileNotFoundError(e):\n","            print(e)\n","            stack_rgb = None\n","            stack_flow = None\n","        stacks['rgb'] = stack_rgb\n","        stacks['flow'] = stack_flow\n","\n","    if 'caption_features' in feature_names_list:\n","        try:\n","            stack_captions = np.load(os.path.join(FEATURE_PATH, f'{video_id}_manual_cc_embedding.npy'))\n","            stack_captions = torch.from_numpy(stack_captions).float()\n","\n","            if get_full_feat:\n","                stacks['orig_feat_length']['caption'] = stack_captions.shape[0]\n","                if stack_captions.shape[0] > param['FRAMES']:\n","                    stack_captions = crop_a_segment(stack_captions, start_idx = 0, end_idx = param['FRAMES'])\n","                else:\n","                    stack_captions = rep_captions(stack_captions)\n","\n","        except FileNotFoundError as e:\n","            print(e)\n","            stack_captions = None\n","\n","        stacks['caption'] = stack_captions\n","\n","    if 'title_features' in feature_names_list:\n","        try:\n","            stack_title = np.load(os.path.join(FEATURE_PATH, f'{video_id}_title.npy'))\n","            stack_title = stack_title.reshape(1, -1)\n","            stack_title = torch.from_numpy(stack_title).float()\n","\n","            if get_full_feat:\n","                stacks['orig_feat_length']['title'] = stack_title.shape[0]\n","                if stack_title.shape[0] > param['FRAMES']:\n","                    stack_title = crop_a_segment(stack_title, start_idx = 0, end_idx = param['FRAMES'])\n","                else:\n","                    stack_title = rep_captions(stack_title)\n","\n","        except FileNotFoundError as e:\n","            print(e)\n","            stack_captions = None\n","\n","        stacks['title'] = stack_title\n","    \n","    else:\n","        raise Exception(f'This methods is not implemented for {feature_names_list}')\n","\n","    # if 'i3d_features' not in feature_names_list and 'vggish_features' not in feature_names_list:\n","    #     raise Exception(f'This methods is not implemented for {feature_names_list}')\n","\n","    return stacks"],"metadata":{"id":"4MRtO0Sh9Zv6","executionInfo":{"status":"ok","timestamp":1654779008382,"user_tz":-120,"elapsed":212,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["class AudioVideoFeaturesDataset(Dataset):\n","    \n","    def __init__(self, \n","                 video_features_path, video_feature_name, \n","                 audio_features_path, audio_feature_name, \n","                 caption_features_path, caption_feature_name,\n","                 title_features_path, title_feature_name,\n","                 device, pad_idx, get_full_feat):\n","      \n","        self.video_features_path = video_features_path\n","        self.video_feature_name = f'{video_feature_name}_features'\n","        self.audio_features_path = audio_features_path\n","        self.audio_feature_name = f'{audio_feature_name}_features'\n","        self.caption_features_path = caption_features_path\n","        self.caption_feature_name = f'{caption_feature_name}_features'\n","        self.title_features_path = title_features_path\n","        self.title_feature_name = f'{title_feature_name}_features'\n","        self.feature_names_list = [self.video_feature_name, \n","                                   self.audio_feature_name,\n","                                   self.caption_feature_name,\n","                                   self.title_feature_name]\n","        self.pad_idx = pad_idx\n","        self.get_full_feat = get_full_feat\n","        self.device = device\n","        \n","        if self.video_feature_name == 'i3d_features':\n","            self.video_feature_size = 1024\n","        else:\n","            raise Exception(f'Inspect: \"{self.video_feature_name}\"')\n","            \n","        if self.audio_feature_name == 'vggish_features':\n","            self.audio_feature_size = 128\n","        else:\n","            raise Exception(f'Inspect: \"{self.audio_feature_name}\"')\n","\n","        if self.caption_feature_name == 'caption_features':\n","            self.cap_feature_size = 512\n","        else:\n","            raise Exception(f'Inspect: \"{self.video_feature_name}\"')\n","\n","        if self.title_feature_name == 'title_features':\n","            self.title_feature_size = 512\n","        else:\n","            raise Exception(f'Inspect: \"{self.video_feature_name}\"')\n","            \n","    \n","    def __getitem__(self, indices):\n","        video_ids = []\n","        vid_stacks_rgb, vid_stacks_flow, aud_stacks, cap_stacks, title_stacks = [], [], [], [], []\n","\n","        if type(indices) == int:\n","            video_id = video_dataset[indices]\n","            \n","            stack = load_features_from_npy(self.feature_names_list, video_id,\n","                           self.pad_idx, self.get_full_feat)\n","            \n","            vid_stack_rgb, vid_stack_flow, aud_stack, cap_stack, title_stack = stack['rgb'], stack['flow'], stack['audio'], stack['caption'], stack['title']\n","\n","            # either both None or both are not None (Boolean Equivalence)\n","            both_are_None = vid_stack_rgb is None and vid_stack_flow is None\n","            none_is_None = vid_stack_rgb is not None and vid_stack_flow is not None\n","            assert both_are_None or none_is_None\n","\n","            # sometimes vid_stack and aud_stack are empty after the filtering. \n","            # we replace it with noise.\n","            # tied with assertion above\n","            if (vid_stack_rgb is None) and (vid_stack_flow is None):\n","                # print(f'RGB and FLOW are None. Zero (1, D) @: {video_id}')\n","                vid_stack_rgb = fill_missing_features('zero', self.video_feature_size)\n","                vid_stack_flow = fill_missing_features('zero', self.video_feature_size)\n","\n","            if aud_stack is None:\n","                # print(f'Audio is None. Zero (1, D) @: {video_id}')\n","                aud_stack = fill_missing_features('zero', self.audio_feature_size)\n","\n","            if cap_stack is None:\n","                # print(f'Caption is None. Zero (1, D) @: {video_id}')\n","                cap_stack = fill_missing_features('zero', self.cap_feature_size)\n","\n","            if title_stack is None:\n","                # print(f'Title is None. Zero (1, D) @: {video_id}')\n","                title_stack = fill_missing_features('zero', self.title_feature_size)\n","\n","            # append info for this index to the lists\n","            video_ids = video_id\n","            vid_stacks_rgb = vid_stack_rgb\n","            vid_stacks_flow = vid_stack_flow\n","            aud_stacks = aud_stack\n","            cap_stacks = cap_stack\n","            title_stacks = title_stack\n","\n","        else: \n","            for idx in range(indices):\n","                video_id = video_dataset[idx]\n","                \n","                stack = load_features_from_npy(self.feature_names_list, video_id,\n","                              self.pad_idx, self.get_full_feat)\n","                \n","                vid_stack_rgb, vid_stack_flow, aud_stack, cap_stack, title_stack = stack['rgb'], stack['flow'], stack['audio'], stack['caption'], stack['title']\n","\n","                # either both None or both are not None (Boolean Equivalence)\n","                both_are_None = vid_stack_rgb is None and vid_stack_flow is None\n","                none_is_None = vid_stack_rgb is not None and vid_stack_flow is not None\n","                assert both_are_None or none_is_None\n","\n","                # sometimes vid_stack and aud_stack are empty after the filtering. \n","                # we replace it with noise.\n","                # tied with assertion above\n","                if (vid_stack_rgb is None) and (vid_stack_flow is None):\n","                    # print(f'RGB and FLOW are None. Zero (1, D) @: {video_id}')\n","                    vid_stack_rgb = fill_missing_features('zero', self.video_feature_size)\n","                    vid_stack_flow = fill_missing_features('zero', self.video_feature_size)\n","\n","                if aud_stack is None:\n","                    # print(f'Audio is None. Zero (1, D) @: {video_id}')\n","                    aud_stack = fill_missing_features('zero', self.audio_feature_size)\n","\n","                if cap_stack is None:\n","                    # print(f'Caption is None. Zero (1, D) @: {video_id}')\n","                    cap_stack = fill_missing_features('zero', self.cap_feature_size)\n","\n","                if title_stack is None:\n","                    # print(f'Title is None. Zero (1, D) @: {video_id}')\n","                    title_stack = fill_missing_features('zero', self.title_feature_size)\n","\n","                # append info for this index to the lists\n","                video_ids.append(video_id)\n","                vid_stacks_rgb.append(vid_stack_rgb)\n","                vid_stacks_flow.append(vid_stack_flow)\n","                aud_stacks.append(aud_stack)\n","                cap_stacks.append(cap_stack)\n","                title_stacks.append(title_stack)\n","            \n","        # [4] see ActivityNetCaptionsDataset.__getitem__ documentation\n","        # padded with pad_idx expected to be summed later\n","        vid_stacks_rgb = pad_sequence(vid_stacks_rgb, batch_first=True, padding_value=self.pad_idx)\n","        vid_stacks_flow = pad_sequence(vid_stacks_flow, batch_first=True, padding_value=self.pad_idx)\n","        aud_stacks = pad_sequence(aud_stacks, batch_first=True, padding_value=self.pad_idx)\n","        cap_stacks = pad_sequence(cap_stacks, batch_first=True, padding_value=self.pad_idx)\n","        title_stacks = pad_sequence(title_stacks, batch_first=True, padding_value=self.pad_idx)\n","                \n","        batch_dict = {\n","            'video_ids': video_ids,\n","            'feature_stacks': {\n","                'rgb': vid_stacks_rgb.to(self.device),\n","                'flow': vid_stacks_flow.to(self.device),\n","                'audio': aud_stacks.to(self.device),\n","                'caption': cap_stacks.to(self.device),\n","                'title': title_stacks.to(self.device),\n","            }\n","        }\n","\n","        return batch_dict\n","        \n","    def __len__(self):\n","        return len(video_dataset)"],"metadata":{"id":"MZ8Ys81CSd4y","executionInfo":{"status":"ok","timestamp":1654779473186,"user_tz":-120,"elapsed":511,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["dataset = AudioVideoFeaturesDataset(FEATURE_PATH, 'i3d', \n","                                    FEATURE_PATH, 'vggish',\n","                                    FEATURE_PATH, 'caption',\n","                                    FEATURE_PATH, 'title',\n","                                    device='cpu', pad_idx=0, get_full_feat=True)\n","\n","train_loader = DataLoader(dataset, batch_size=1)\n","\n","for i, item in enumerate(train_loader):\n","    print(item['video_ids'])\n","    print(item['feature_stacks']['rgb'].shape)\n","    print(item['feature_stacks']['flow'].shape)\n","    print(item['feature_stacks']['audio'].shape)\n","    print(item['feature_stacks']['caption'].shape)\n","    print(item['feature_stacks']['title'].shape)\n","    print('------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fD3kccOoUicM","executionInfo":{"status":"ok","timestamp":1654779456110,"user_tz":-120,"elapsed":229,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"09f3860a-6117-42e1-bf2d-4b348ee46342"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Single Vid\n","['98MoyGZKHXc']\n","torch.Size([1, 179, 1024])\n","torch.Size([1, 179, 1024])\n","torch.Size([1, 179, 128])\n","torch.Size([1, 179, 512])\n","torch.Size([1, 179, 512])\n","------------------------------------\n"]}]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Wide multi-head self-attention layer.\n","\n","    Args:\n","        k: embedding dimension\n","        heads: number of heads (k mod heads must be 0)\n","        dims: dimensions of the key, query and value embeddings\n","\n","    \"\"\"\n","    def __init__(self, k, heads=8, **dims):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        self.heads = heads\n","        self.k = k\n","\n","        # These compute the queries, keys and values for all \n","        # heads (as a single concatenated vector)\n","        self.tokeys    = nn.Linear(dims['k_dim'], k * heads, bias=False)\n","        self.toqueries = nn.Linear(dims['q_dim'], k * heads, bias=False)\n","        self.tovalues  = nn.Linear(dims['v_dim'], k * heads, bias=False)\n","\n","        # This unifies the outputs of the different heads into \n","        # a single k-vector\n","        self.unifyheads = nn.Linear(k * heads, k)\n","        \n","    def forward(self, q_in, k_in, v_in):\n","\n","        b, q_t, _ = q_in.size()\n","        _, k_t, _ = k_in.size()\n","        _, v_t, _ = v_in.size()\n","        h = self.heads\n","\n","        # Project input to queries, keys and values\n","        queries = self.toqueries(q_in).view(b, q_t, self.heads, self.k)\n","        keys    = self.tokeys(k_in).view(b, k_t, self.heads, self.k)\n","        values  = self.tovalues(v_in).view(b, v_t, self.heads, self.k)\n","\n","        # Fold heads into the batch dimension\n","        keys = keys.transpose(1, 2).reshape(b * self.heads, k_t, self.k)\n","        queries = queries.transpose(1, 2).reshape(b * self.heads, q_t, self.k)\n","        values = values.transpose(1, 2).reshape(b * self.heads, v_t, self.k)\n","        \n","        # Compute attention weights\n","        w_prime = torch.bmm(queries, keys.transpose(1, 2))\n","        w_prime = w_prime / (self.k ** (1 / 2))\n","        w = F.softmax(w_prime, dim=2)\n","\n","        # Apply the self-attention to the values\n","        y = torch.bmm(w, values).view(b, self.heads, q_t, self.k)\n","\n","        # Swap h, t back, unify heads\n","        y = y.transpose(1, 2).reshape(b, q_t, self.heads * self.k)\n","\n","        y = self.unifyheads(y)\n","\n","        return y"],"metadata":{"id":"uIgQUl4Z62w8","executionInfo":{"status":"ok","timestamp":1654779031486,"user_tz":-120,"elapsed":207,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Load dataset and create dataloader\n","dataset = AudioVideoFeaturesDataset(FEATURE_PATH, 'i3d', \n","                                    FEATURE_PATH, 'vggish',\n","                                    FEATURE_PATH, 'caption',\n","                                    FEATURE_PATH, 'title',\n","                                    device='cpu', pad_idx=0, get_full_feat=True)\n","\n","train_loader = DataLoader(dataset, batch_size=1)"],"metadata":{"id":"jX_dThkAqxfk","executionInfo":{"status":"ok","timestamp":1654780616989,"user_tz":-120,"elapsed":234,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# Level 1: Apply multi-headed attention on video and audio features\n","\n","aud_vid_module = MultiHeadAttention(k = 512, heads=3, q_dim=1024, v_dim=1024, k_dim=128)\n","\n","attended_features = []\n","\n","for i, item in enumerate(train_loader):\n","    print(item['video_ids'])\n","\n","    vid = item['feature_stacks']['rgb'] + item['feature_stacks']['flow']\n","    aud = item['feature_stacks']['audio']\n","    y = aud_vid_module(vid, aud, vid)\n","\n","    attended_features.append(y)\n","\n","attended_vid = torch.stack(attended_features) # attended features for audio and video\n","\n","torch.save(attended_vid, f='/content/drive/MyDrive/MM/temp_features/attended_vid.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dI-k_xmfgmhm","executionInfo":{"status":"ok","timestamp":1654780618543,"user_tz":-120,"elapsed":223,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"154ad1fe-92ba-4f77-8c71-fb0138323568"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["['98MoyGZKHXc']\n"]}]},{"cell_type":"code","source":["# Level 2: Apply multi-headed attention on attended video features and caption features\n","\n","vid_caption_module = MultiHeadAttention(k = 512, heads=3, q_dim=512, v_dim=512, k_dim=512)\n","\n","attended_features = []\n","\n","for i, item in enumerate(train_loader):\n","    print(item['video_ids'])\n","\n","    vid = attended_vid[i]\n","    caption = item['feature_stacks']['caption']\n","    y = vid_caption_module(vid, caption, vid)\n","\n","    attended_features.append(y) # attended features of audio, video and captions\n","\n","attended_vid_cap = torch.stack(attended_features)\n","torch.save(attended_vid, f='/content/drive/MyDrive/MM/temp_features/attended_vid_cap.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KldyBLkhEu_","executionInfo":{"status":"ok","timestamp":1654780626882,"user_tz":-120,"elapsed":218,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"497ef6b7-d8ae-442f-911a-e2dee9d67025"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["['98MoyGZKHXc']\n"]}]},{"cell_type":"code","source":["# Level 3: Apply multi-headed attention on attended video and video title features\n","\n","vid_caption_module = MultiHeadAttention(k = 512, heads=3, q_dim=512, v_dim=512, k_dim=512)\n","\n","attended_features = []\n","\n","for i, item in enumerate(train_loader):\n","    print(item['video_ids'])\n","\n","    vid = attended_vid_cap[i]\n","    title = item['feature_stacks']['title']\n","    y = vid_caption_module(vid, title, vid)\n","\n","    attended_features.append(y) # attended features audio, video, captions and video title\n","\n","attended_vid_cap_title = torch.stack(attended_features)\n","torch.save(attended_vid, f='/content/drive/MyDrive/MM/temp_features/attended_vid_cap_title.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6y055OIniDew","executionInfo":{"status":"ok","timestamp":1654780639965,"user_tz":-120,"elapsed":241,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"03b23bcd-6862-4f9d-af71-6ed829247024"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["['98MoyGZKHXc']\n"]}]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, k, num_outputs):\n","        \"\"\"\n","        Basic transformer.\n","\n","        Args:\n","            k: embedding dimension\n","            num_outputs: outputs of the final layer (= num of frames)\n","        \"\"\"\n","        super(Transformer, self).__init__()\n","\n","        # self.att = MultiHeadAttention(k, heads=heads)\n","\n","        self.norm1 = nn.LayerNorm(k)\n","\n","        self.ff = nn.Sequential(\n","            nn.Linear(k, 4 * k),\n","            nn.ReLU(),\n","            nn.Linear(4 * k, k))\n","        \n","        self.norm2 = nn.LayerNorm(k)\n","\n","        self.output_projector = nn.Linear(k, num_outputs)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of trasformer block.\n","\n","        Args:\n","            x: input with shape of (b, k)\n","        \n","        Returns:\n","            y: output with shape of (b, k)\n","        \"\"\"\n","\n","        # Normalize\n","        x = self.norm1(x)\n","\n","        # Pass through feed-forward network\n","        y = self.ff(x)\n","\n","        # Second residual connection\n","        x = x + y\n","\n","        # Again normalize\n","        y = self.norm2(x)\n","\n","        # Average-pool over dimension t\n","        y = y.mean(dim=1)\n","\n","        # Project output to desired size\n","        y = self.output_projector(y)\n","\n","        # Softmax on the last layer for relevance scores\n","        y = F.softmax(y, dim=1)\n","\n","        return y"],"metadata":{"id":"u_TGAbpuAN1G","executionInfo":{"status":"ok","timestamp":1654781302720,"user_tz":-120,"elapsed":3,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Final block: apply transformer on the hierarchially attended features\n","\n","transformer = Transformer(k=512, num_outputs=param['FRAMES'])\n","\n","outputs = []\n","\n","for i, item in enumerate(train_loader):\n","    print(item['video_ids'])\n","\n","    vid = attended_vid_cap_title[i]\n","    y = transformer(vid)\n","\n","    outputs.append(y)\n","\n","outputs = torch.stack(outputs) # outputs of the model\n","\n","torch.save(outputs, f='/content/drive/MyDrive/MM/temp_features/outputs.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D342WzIOoVRc","executionInfo":{"status":"ok","timestamp":1654781305000,"user_tz":-120,"elapsed":21,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"9d0b2dcb-dc96-47cb-ce31-0f4ddaa18b17"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["['98MoyGZKHXc']\n"]}]},{"cell_type":"code","source":["pred = outputs[0][0]\n","pred = pred.detach().cpu().numpy()\n","\n","# normalize the predictions\n","pred_norm = (pred - pred.min())/ (pred.max() - pred.min())"],"metadata":{"id":"Dp_Ly1i4ql5s","executionInfo":{"status":"ok","timestamp":1654782637353,"user_tz":-120,"elapsed":213,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["annotations = pd.read_csv('/content/drive/MyDrive/MM/temp_features/annotations.csv')\n","\n","labels = np.array(annotations['labels'])\n","\n","# normalize the lables as it is in a range of 1-5 and we are interested in \n","# frame-wise binary relevance classification\n","labels_norm = (labels - labels.min())/(labels.max() - labels.min())"],"metadata":{"id":"jEye6GOKueE_","executionInfo":{"status":"ok","timestamp":1654782914727,"user_tz":-120,"elapsed":210,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["# plot the overlapping relevance predicition and ground truth scores\n","\n","fig, ax = plt.subplots(figsize=(20,5))\n","ax.bar(range(len(labels_norm)), labels_norm, label='Ground Truth')\n","ax.bar(range(len(pred_norm)), pred_norm, label='Predictions')\n","plt.title('Frame-wise Relevance')\n","\n","plt.xlabel('Frames')\n","plt.ylabel('Normalised Relevance Scores')\n","plt.legend()\n","plt.savefig('/content/drive/MyDrive/MM/temp_features/rel_score.png', dpi=300)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"0nEpP9-FuHko","executionInfo":{"status":"ok","timestamp":1654785526680,"user_tz":-120,"elapsed":3382,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"827082fe-6166-4bdc-d487-6f775f667d53"},"execution_count":172,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Apply threshold to convert continuos scores to binary labels\n","\n","THRESHOLD = 0.3\n","\n","label_gt = np.array(labels_norm > THRESHOLD).astype('int')\n","label_pred = np.array(pred_norm > THRESHOLD).astype('int')\n","\n","print(f'Accuracy score: {round(accuracy_score(label_gt, label_pred), 2)}')\n","print(f'Precision score: {round(precision_score(label_gt, label_pred), 2)}')\n","print(f'Recall score: {round(recall_score(label_gt, label_pred), 2)}')\n","print(f'F1 score: {round(f1_score(label_gt, label_pred), 2)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrLqSrwzy-yb","executionInfo":{"status":"ok","timestamp":1654785659205,"user_tz":-120,"elapsed":231,"user":{"displayName":"Sayak Mukherjee","userId":"10046678833889952329"}},"outputId":"dbe7bff7-ecf1-4adb-84e0-e07970f3c9f8"},"execution_count":175,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score: 0.52\n","Precision score: 0.38\n","Recall score: 0.24\n","F1 score: 0.3\n"]}]}]}