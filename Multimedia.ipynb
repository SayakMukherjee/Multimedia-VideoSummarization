{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multimedia.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#Uncomment for colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"u_bt0G2b2e6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"xVGktDYo9C4m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5kLDG0IqjK1"},"outputs":[],"source":["%%capture\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","source":["%%capture\n","!git clone --recursive https://github.com/SayakMukherjee/BMT.git\n","%cd BMT/\n","\n","!wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh -q --show-progress\n","!bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local"],"metadata":{"id":"mVWZWhehrkg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","import sys\n","sys.path.append('/usr/local/lib/python3.7/site-packages/')\n","from sample.single_video_prediction import get_video_duration"],"metadata":{"id":"CU2qFmNJsAZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# feature extraction\n","!conda env create -f ./submodules/video_features/conda_env_i3d.yml"],"metadata":{"id":"IkQcQ2omsEH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!conda env create -f ./submodules/video_features/conda_env_vggish.yml"],"metadata":{"id":"BxgoXzcUvTFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# captioning model\n","!conda env create -f ./conda_env.yml\n","\n","# spacy language model\n","!/usr/local/envs/bmt/bin/python -m spacy download en"],"metadata":{"id":"-H1dQUjnrcKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/glove.840B.300d.zip -q --show-progress\n","!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_cap_model.pt -q --show-progress\n","!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_prop_model.pt -q --show-progress\n","!wget https://storage.googleapis.com/audioset/vggish_model.ckpt -q --show-progress\n","\n","!mkdir .vector_cache\n","!mv glove.840B.300d.zip ./.vector_cache/\n","!mv best_cap_model.pt ./sample/\n","!mv best_prop_model.pt ./sample/\n","!mv vggish_model.ckpt ./submodules/video_features/models/vggish/checkpoints/"],"metadata":{"id":"AUjZN-FXsau1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Caption Generation: Bi-Modal Tranformer\n","\n","Adapted from the original work by Iashin et al. [Bi-modal Transfor](https://github.com/v-iashin/BMT)"],"metadata":{"id":"Tu4co1vK9RYa"}},{"cell_type":"code","source":["# upload a video\n","MY_VIDEO_PATH = '/content/drive/MyDrive/MM/tvsum/video/98MoyGZKHXc.mp4'\n","\n","# Preparing the paths\n","VIDEO_DURATION = get_video_duration(MY_VIDEO_PATH)\n","\n","FEATURES_CACHE_PATH = '/content/drive/MyDrive/MM/tvsum/features'\n","FEATURES_PATH_STUB = os.path.join(FEATURES_CACHE_PATH, Path(MY_VIDEO_PATH).stem)\n","\n","FEATURE_PATH_RGB = f'{FEATURES_PATH_STUB}_rgb.npy'\n","FEATURE_PATH_FLOW = f'{FEATURES_PATH_STUB}_flow.npy'"],"metadata":{"id":"xPGxlv-CseR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PROPOSAL_CKPT = '/content/BMT/sample/best_prop_model.pt'\n","CAPTIONING_CKPT = '/content/BMT/sample/best_cap_model.pt'"],"metadata":{"id":"tviNCcIf9D-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FEATURE_PATH_VGGISH = f'{FEATURES_PATH_STUB}_vggish.npy'"],"metadata":{"id":"ULQRBIHPxM5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract I3D features (visual)\n","!cd ./submodules/video_features && /usr/local/envs/i3d/bin/python main.py \\\n","    --feature_type i3d \\\n","    --on_extraction save_numpy \\\n","    --device_ids 0 \\\n","    --extraction_fps 1 \\\n","    --step_size 1 \\\n","    --stack_size 10 \\\n","    --video_paths $MY_VIDEO_PATH \\\n","    --output_path $FEATURES_CACHE_PATH"],"metadata":{"id":"_qQ_zLU12Mql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract VGGish features (audio)\n","!cd ./submodules/video_features && /usr/local/envs/vggish/bin/python main.py \\\n","    --feature_type vggish \\\n","    --on_extraction save_numpy \\\n","    --device_ids 0 \\\n","    --video_paths $MY_VIDEO_PATH \\\n","    --output_path $FEATURES_CACHE_PATH"],"metadata":{"id":"zi-2rJg_xad-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# captioning parameters\n","MAX_PROP_PER_VIDEO = 100\n","NMS_TIOU_THRESHOLD = 0.4\n","\n","# Running single video prediction\n","!/usr/local/envs/bmt/bin/python ./sample/single_video_prediction.py \\\n","    --prop_generator_model_path $PROPOSAL_CKPT \\\n","    --pretrained_cap_model_path $CAPTIONING_CKPT \\\n","    --vggish_features_path '/content/drive/MyDrive/MM/tvsum/features/-esJrBWj2d8_vggish.npy' \\\n","    --rgb_features_path '/content/drive/MyDrive/MM/tvsum/new_features/-esJrBWj2d8_rgb.npy' \\\n","    --flow_features_path '/content/drive/MyDrive/MM/tvsum/new_features/-esJrBWj2d8_flow.npy' \\\n","    --duration_in_secs $VIDEO_DURATION \\\n","    --device_id 0 \\\n","    --max_prop_per_vid $MAX_PROP_PER_VIDEO \\\n","    --nms_tiou_thresh $NMS_TIOU_THRESHOLD"],"metadata":{"id":"kbkVKccW4C5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Captions Encoding\n","\n","Adapted from the text encoder section of the work by Radford et al. [Contrastive Language-Image Pre-Training (CLIP)](https://github.com/openai/CLIP)"],"metadata":{"id":"SvWyxyA7HmHy"}},{"cell_type":"code","source":["# Hand-written captions as BMT needs fine-tuning on TVSum which is beyond the current scope\n","\n","captions_list = [{'start': 0.0, 'end': 20.0, 'sentence': 'man is talking about repair kits in cars'}, \n","                 {'start': 20.0, 'end': 40.0, 'sentence': 'man stops car and gets out of the car and sits on road and talks to the camera'}, \n","                 {'start': 40.0, 'end': 60.0, 'sentence': 'man is talking to the camera and touches the wheels and the back of the car'}, \n","                 {'start': 60.0, 'end': 80.0, 'sentence': 'man is talking to the camera and takes out items from the car back and sits and touches wheels'}, \n","                 {'start': 80.0, 'end': 100.0, 'sentence': 'man is talking to the camera and touches the wheels then he gets up and opens car front door'}, \n","                 {'start': 100.0, 'end': 120.0, 'sentence': 'a personâ€™s hand and finger and wheel and car starts'}, \n","                 {'start': 120.0, 'end': 140.0, 'sentence': 'car wheels and car starts and man is driving car and talking to the camera'}, \n","                 {'start': 140.0, 'end': 160.0, 'sentence': 'man is talking to the camera and car is going forward on the road'}]\n","captions_list[0]"],"metadata":{"id":"gH6QJ60jHrLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sorting the captions to ensure there is no overlap\n","newlist = sorted(captions_list, key=lambda d: d['start']) \n","newlist\n","list_of_sentences = []\n","for item in newlist:\n","  list_of_sentences.append(item['sentence'])"],"metadata":{"id":"NhKhiK6VHwsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_of_sentences # print"],"metadata":{"id":"D-tcHpltLqbu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizing the list of captions using CLIP\n","\n","import torch\n","import clip\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","text = clip.tokenize(list_of_sentences).to(device)\n","\n","with torch.no_grad():\n","    text_features = model.encode_text(text)"],"metadata":{"id":"nYcXrfdNq0bZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gpu to cpu\n","\n","from torch.functional import Tensor\n","import numpy as np\n","\n","np_array_from_gpu = Tensor.cpu(text_features)\n","np.save('/content/drive/MyDrive/MM/tvsum/features/98MoyGZKHXc_manual_cc_embedding.npy', np_array_from_gpu)"],"metadata":{"id":"BLG33eg7rJpY"},"execution_count":null,"outputs":[]}]}