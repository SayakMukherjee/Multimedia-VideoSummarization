# Hierarchial Context Aware Multi-modal Video Summarization

Video content generation and consumption have seen an exponential growth in recent years. The massive amount of video data on the web is a stark contrast to limited browsing time at the hands of users who want to look for the relevant information as quickly as possible. This exacerbates the need of effective video summarization leading to a growing interest in the corresponding research domain. Majority of the works currently focus on using video and audio modalities or video along with captions to formulate the video summary. However, intuitively each modality on its own provides a different perspective of information, so more information about the context can prove to be beneficial in order to draft a more effective summary. Furthermore in the previous works, the textual features have been based solely on the video caption embeddings. Therefore, in this research, we propose to use audio and visual features along with textual embeddings to incorporate both the video title as well as the captions. <br> <br>
Following the previous works, it is often not evident how a particular modality is influencing the summary and therefore how important each of the modalities are for an enriched and meaningful summary.  It is interesting because adding more features is often more expensive in terms of resources. Therefore to analyse the importance of each of the modalities, we use deep metric learning. This would be discussed in more detail in the later sections of the paper. This research is fundamental as it can serve as a starting point to the field to help drive towards more effective and less costly summaries by using only the components which provide important insights.